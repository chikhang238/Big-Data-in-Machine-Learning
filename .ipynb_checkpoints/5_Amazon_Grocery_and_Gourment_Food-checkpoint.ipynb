{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sc = SparkContext(master = 'local', appName = 'amazon grocery and gourment food')\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"ratings_Grocery_and_Gourmet_Food.csv\", header = False, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---+----------+\n",
      "|           _c0|       _c1|_c2|       _c3|\n",
      "+--------------+----------+---+----------+\n",
      "|A1ZQZ8RJS1XVTX|0657745316|5.0|1381449600|\n",
      "|A31W38VGZAUUM4|0700026444|5.0|1354752000|\n",
      "|A3I0AV0UJX5OH0|1403796890|1.0|1385942400|\n",
      "|A3QAAOLIXKV383|1403796890|3.0|1307836800|\n",
      "| AB1A5EGHHVA9M|141278509X|5.0|1332547200|\n",
      "+--------------+----------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       _c1|\n",
      "+----------+\n",
      "|B0000D1699|\n",
      "|B0000D17HO|\n",
      "|B0000D89E8|\n",
      "|B0000D94PL|\n",
      "|B0000DC3QJ|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('_c1').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---+----------+\n",
      "|           _c0|       _c1|_c2|       _c3|\n",
      "+--------------+----------+---+----------+\n",
      "|A2BSUJYATHI7WW|B002YRBALU|4.0|1395619200|\n",
      "|A2BSUJYATHI7WW|B007GPARA0|4.0|1395619200|\n",
      "+--------------+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(data['_c0'] == 'A2BSUJYATHI7WW').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('_c0', '_c2', '_c1').withColumnRenamed('_c0', 'reviewerID').withColumnRenamed('_c2', 'overall').withColumnRenamed('_c1', 'asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------+\n",
      "|    reviewerID|overall|      asin|\n",
      "+--------------+-------+----------+\n",
      "|A1ZQZ8RJS1XVTX|    5.0|0657745316|\n",
      "|A31W38VGZAUUM4|    5.0|0700026444|\n",
      "|A3I0AV0UJX5OH0|    1.0|1403796890|\n",
      "|A3QAAOLIXKV383|    3.0|1403796890|\n",
      "| AB1A5EGHHVA9M|    5.0|141278509X|\n",
      "+--------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+\n",
      "|reviewerID|overall|asin|\n",
      "+----------+-------+----+\n",
      "|         0|      0|   0|\n",
      "+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data.select([count(when(isnan(col), col)).alias(col) for col in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+\n",
      "|reviewerID|overall|asin|\n",
      "+----------+-------+----+\n",
      "|         0|      0|   0|\n",
      "+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select([count(when(isnull(col), col)).alias(col) for col in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count() - data.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_users = data.select('reviewerID').distinct().count()\n",
    "number_of_products = data.select('asin').distinct().count()\n",
    "number_of_ratings = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768438"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166049"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999898340700841"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity = 1 - (number_of_ratings/(number_of_users*number_of_products))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer1 = StringIndexer(inputCol = 'asin', outputCol = 'asin_idx')\n",
    "indexer2 = StringIndexer(inputCol = 'reviewerID', outputCol = 'reviewerID_idx')\n",
    "\n",
    "data_indexed = indexer1.fit(data).transform(data)\n",
    "data_indexed = indexer2.fit(data_indexed).transform(data_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------+--------+--------------+\n",
      "|reviewerID    |overall|asin      |asin_idx|reviewerID_idx|\n",
      "+--------------+-------+----------+--------+--------------+\n",
      "|A1ZQZ8RJS1XVTX|5.0    |0657745316|122279.0|165912.0      |\n",
      "|A31W38VGZAUUM4|5.0    |0700026444|157556.0|750032.0      |\n",
      "|A3I0AV0UJX5OH0|1.0    |1403796890|88468.0 |201813.0      |\n",
      "|A3QAAOLIXKV383|3.0    |1403796890|88468.0 |45441.0       |\n",
      "|AB1A5EGHHVA9M |5.0    |141278509X|127327.0|758035.0      |\n",
      "+--------------+-------+----------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_indexed.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "train,test = data_indexed.randomSplit([0.8, 0.2])\n",
    "\n",
    "als = ALS(maxIter = 10, regParam = 0.1, rank = 25,\n",
    "         userCol = \"reviewerID_idx\",\n",
    "         itemCol = \"asin_idx\",\n",
    "         ratingCol = \"overall\", coldStartStrategy = \"drop\", nonnegative = True)\n",
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|overall|prediction|\n",
      "+-------+----------+\n",
      "|    5.0| 3.8570075|\n",
      "|    5.0| 3.2644181|\n",
      "|    5.0| 3.7489128|\n",
      "|    2.0| 1.3912433|\n",
      "|    4.0| 1.7047215|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select('overall', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7156368893410001"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName = 'rmse',\n",
    "                               labelCol = 'overall',\n",
    "                               predictionCol = \"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With maxiter = 20, I got the result which is 1.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ALS_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = ALSModel.load('ALS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs = model_new.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "recs=model_new.recommendForAllUsers(10).toPandas()\n",
    "nrecs=recs.recommendations.apply(pd.Series) \\\n",
    "            .merge(recs, right_index = True, left_index = True) \\\n",
    "            .drop([\"recommendations\"], axis = 1) \\\n",
    "            .melt(id_vars = ['reviewerID_idx'], value_name = \"recommendation\") \\\n",
    "            .drop(\"variable\", axis = 1) \\\n",
    "            .dropna() \n",
    "nrecs=nrecs.sort_values('reviewerID_idx')\n",
    "nrecs=pd.concat([nrecs['recommendation'].apply(pd.Series), nrecs['reviewerID_idx']], axis = 1)\n",
    "nrecs.columns = [\n",
    "        'ProductID_idx',\n",
    "        'Rating',\n",
    "        'UserID_idx'\n",
    "       ]\n",
    "md=transformed.select(transformed['reviewerID'],transformed['reviewerID_idx'],transformed['asin'],transformed['asin_idx'])\n",
    "md=md.toPandas()\n",
    "dict1 =dict(zip(md['reviewerID_idx'],md['reviewerID']))\n",
    "dict2=dict(zip(md['asin_idx'],md['asin']))\n",
    "nrecs['reviewerID']=nrecs['UserID_idx'].map(dict1)\n",
    "nrecs['asin']=nrecs['ProductID_idx'].map(dict2)\n",
    "nrecs=nrecs.sort_values('reviewerID')\n",
    "nrecs.reset_index(drop=True, inplace=True)\n",
    "new=nrecs[['reviewerID','asin','Rating']]\n",
    "new['recommendations'] = list(zip(new.asin, new.Rating))\n",
    "res=new[['reviewerID','recommendations']]  \n",
    "res_new=res['recommendations'].groupby([res.reviewerID]).apply(list).reset_index()\n",
    "print(res_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained this model 3 times and obtained the results with RMSE being 1.59, 1.63, 1.71 and make predictions.\n",
    "However, it ran out of my resources when I tried to convert dataframe of recommendation for all users. So, I can not give the final results due to my weak computer. However, I have codes for the chance that we can build the system which have master computer and slave computers.\n",
    "\n",
    "The above codes can give us the pandas dataframe which provides reviewerID and recommendations. Furthermore, recommendations column contains tuples of products and predicted ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for reviewer: A2BSUJYATHI7WW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for customer: A26LKBXTSIHQV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for customer: A3ABZBEG3KZ0L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of above code, we can find out tuples of ratings and related products"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
